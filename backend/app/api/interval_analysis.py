from fastapi import APIRouter, Depends, HTTPException, status
from sqlalchemy.orm import Session
from typing import List, Dict, Any
import logging
from app.core.database import get_db
from app.core.security import get_current_user_from_token
from app.models.workout_import_data import WorkoutImportData, IntervalAnalysis
from app.services.interval_analysis import IntervalAnalyzer
from app.schemas.interval_analysis import (
    IntervalAnalysisRequest,
    IntervalAnalysisResponse,
    CorrectionApplyRequest,
    CorrectionApplyResponse
)

logger = logging.getLogger(__name__)
router = APIRouter()


@router.post("/analyze", response_model=IntervalAnalysisResponse)
async def analyze_interval_data(
    request: IntervalAnalysisRequest,
    current_user_id: str = Depends(get_current_user_from_token),
    db: Session = Depends(get_db)
):
    """ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒ«ãƒ‡ãƒ¼ã‚¿ã®åˆ†æ"""
    try:
        logger.info(f"ğŸ” ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒ«åˆ†æé–‹å§‹: user_id={current_user_id}")
        
        # ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒ«åˆ†æã®å®Ÿè¡Œ
        analyzer = IntervalAnalyzer()
        analysis_result = analyzer.analyze_interval_data(
            request.lap_times,
            request.lap_distances
        )
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œè¨¼
        pattern_validation = analyzer.validate_interval_pattern(
            request.lap_times,
            request.lap_distances
        )
        
        # åˆ†æçµæœã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜
        db_analysis = IntervalAnalysis(
            workout_import_data_id=request.workout_import_data_id,
            total_laps=analysis_result['total_laps'],
            average_lap_time=analysis_result['average_lap_time'],
            average_lap_distance=analysis_result['average_lap_distance'],
            has_anomaly=analysis_result['has_anomaly'],
            anomaly_type=analysis_result['anomaly_type'],
            anomaly_lap_index=analysis_result['anomaly_lap_index'],
            anomaly_severity=analysis_result['anomaly_severity'],
            lap_times=analysis_result['lap_times'],
            lap_distances=analysis_result['lap_distances'],
            lap_paces=analysis_result['lap_paces'],
            suggested_corrections=analysis_result['suggested_corrections']
        )
        
        db.add(db_analysis)
        db.commit()
        db.refresh(db_analysis)
        
        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®æ§‹ç¯‰
        response_data = {
            'analysis_id': db_analysis.id,
            'total_laps': analysis_result['total_laps'],
            'average_lap_time': analysis_result['average_lap_time'],
            'average_lap_distance': analysis_result['average_lap_distance'],
            'has_anomaly': analysis_result['has_anomaly'],
            'anomaly_type': analysis_result['anomaly_type'],
            'anomaly_lap_index': analysis_result['anomaly_lap_index'],
            'anomaly_severity': analysis_result['anomaly_severity'],
            'lap_times': analysis_result['lap_times'],
            'lap_distances': analysis_result['lap_distances'],
            'lap_paces': analysis_result['lap_paces'],
            'suggested_corrections': analysis_result['suggested_corrections'],
            'pattern_validation': pattern_validation,
            'analysis_metadata': analysis_result['analysis_metadata']
        }
        
        logger.info(f"âœ… ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒ«åˆ†æå®Œäº†: ç•°å¸¸={analysis_result['has_anomaly']}")
        return response_data
        
    except Exception as e:
        db.rollback()
        logger.error(f"âŒ ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒ«åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Failed to analyze interval data"
        )


@router.post("/apply-correction", response_model=CorrectionApplyResponse)
async def apply_correction(
    request: CorrectionApplyRequest,
    current_user_id: str = Depends(get_current_user_from_token),
    db: Session = Depends(get_db)
):
    """ä¿®æ­£ã®é©ç”¨"""
    try:
        logger.info(f"ğŸ” ä¿®æ­£é©ç”¨é–‹å§‹: user_id={current_user_id}, correction_type={request.correction_type}")
        
        # å…ƒãƒ‡ãƒ¼ã‚¿ã®å–å¾—
        import_data = db.query(WorkoutImportData).filter(
            WorkoutImportData.id == request.workout_import_data_id,
            WorkoutImportData.user_id == current_user_id
        ).first()
        
        if not import_data:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="Import data not found"
            )
        
        # ä¿®æ­£ã®é©ç”¨
        analyzer = IntervalAnalyzer()
        corrected_times, corrected_distances = analyzer.apply_correction(
            import_data.raw_data.get('lap_times', []),
            import_data.raw_data.get('lap_distances', []),
            request.correction_type
        )
        
        # ä¿®æ­£ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜
        processed_data = import_data.raw_data.copy()
        processed_data['lap_times'] = corrected_times
        processed_data['lap_distances'] = corrected_distances
        
        import_data.processed_data = processed_data
        import_data.modifications = {
            'correction_type': request.correction_type,
            'applied_at': None,  # å®Ÿéš›ã®å®Ÿè£…ã§ã¯ç¾åœ¨æ™‚åˆ»ã‚’è¨­å®š
            'original_lap_count': len(import_data.raw_data.get('lap_times', [])),
            'corrected_lap_count': len(corrected_times)
        }
        
        db.commit()
        
        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®æ§‹ç¯‰
        response_data = {
            'workout_import_data_id': import_data.id,
            'correction_applied': True,
            'correction_type': request.correction_type,
            'original_lap_count': len(import_data.raw_data.get('lap_times', [])),
            'corrected_lap_count': len(corrected_times),
            'corrected_times': corrected_times,
            'corrected_distances': corrected_distances,
            'modifications': import_data.modifications
        }
        
        logger.info(f"âœ… ä¿®æ­£é©ç”¨å®Œäº†: {len(corrected_times)}ãƒ©ãƒƒãƒ—")
        return response_data
        
    except HTTPException:
        raise
    except Exception as e:
        db.rollback()
        logger.error(f"âŒ ä¿®æ­£é©ç”¨ã‚¨ãƒ©ãƒ¼: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Failed to apply correction"
        )


@router.get("/analysis/{analysis_id}", response_model=IntervalAnalysisResponse)
async def get_analysis_result(
    analysis_id: str,
    current_user_id: str = Depends(get_current_user_from_token),
    db: Session = Depends(get_db)
):
    """åˆ†æçµæœã®å–å¾—"""
    try:
        analysis = db.query(IntervalAnalysis).join(WorkoutImportData).filter(
            IntervalAnalysis.id == analysis_id,
            WorkoutImportData.user_id == current_user_id
        ).first()
        
        if not analysis:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="Analysis not found"
            )
        
        return {
            'analysis_id': analysis.id,
            'total_laps': analysis.total_laps,
            'average_lap_time': analysis.average_lap_time,
            'average_lap_distance': analysis.average_lap_distance,
            'has_anomaly': analysis.has_anomaly,
            'anomaly_type': analysis.anomaly_type,
            'anomaly_lap_index': analysis.anomaly_lap_index,
            'anomaly_severity': analysis.anomaly_severity,
            'lap_times': analysis.lap_times,
            'lap_distances': analysis.lap_distances,
            'lap_paces': analysis.lap_paces,
            'suggested_corrections': analysis.suggested_corrections,
            'pattern_validation': {
                'pattern_type': 'unknown',
                'is_valid': True,
                'description': 'åˆ†ææ¸ˆã¿ãƒ‡ãƒ¼ã‚¿'
            },
            'analysis_metadata': {
                'confidence': 0.9,
                'description': 'ä¿å­˜æ¸ˆã¿ã®åˆ†æçµæœ',
                'analysis_timestamp': analysis.analysis_timestamp.isoformat() if analysis.analysis_timestamp else None
            }
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ åˆ†æçµæœå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Failed to fetch analysis result"
        )


@router.post("/set-user-choice")
async def set_user_choice(
    workout_import_data_id: str,
    user_choice: str,
    current_user_id: str = Depends(get_current_user_from_token),
    db: Session = Depends(get_db)
):
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®é¸æŠï¼ˆå…ƒãƒ‡ãƒ¼ã‚¿ vs ä¿®æ­£ãƒ‡ãƒ¼ã‚¿ï¼‰ã‚’è¨­å®š"""
    try:
        if user_choice not in ['raw', 'processed']:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="User choice must be 'raw' or 'processed'"
            )
        
        import_data = db.query(WorkoutImportData).filter(
            WorkoutImportData.id == workout_import_data_id,
            WorkoutImportData.user_id == current_user_id
        ).first()
        
        if not import_data:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="Import data not found"
            )
        
        import_data.user_choice = user_choice
        db.commit()
        
        return {
            'message': f'User choice set to {user_choice}',
            'workout_import_data_id': workout_import_data_id,
            'user_choice': user_choice
        }
        
    except HTTPException:
        raise
    except Exception as e:
        db.rollback()
        logger.error(f"âŒ ãƒ¦ãƒ¼ã‚¶ãƒ¼é¸æŠè¨­å®šã‚¨ãƒ©ãƒ¼: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Failed to set user choice"
        )
